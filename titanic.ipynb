{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/somewhatjustin/titanic?scriptVersionId=164018831\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# TODO: Add Cross-Validation for depth","metadata":{"execution":{"iopub.status.busy":"2024-02-23T15:27:40.750035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# Fill NaNs without using inplace=True\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\n\n# Fit and transform the data for training and testing datasets\ntrain_data['Age_normalized'] = scaler.fit_transform(train_data[['Age']].values.reshape(-1, 1))\ntest_data['Age_normalized'] = scaler.transform(test_data[['Age']].values.reshape(-1, 1))\n\n# Note: Use scaler.fit_transform() on the training data to fit the scaler and transform the data.\n# Use scaler.transform() on the test data to apply the same scaling based on the training data.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Fare Scaler\nfareScaler = MinMaxScaler(feature_range=(0,1))\n\n# Fill NaNs without using inplace=True\ntrain_data['Fare'] = train_data['Fare'].fillna(train_data['Fare'].mean())\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].mean())\n\n# Fit and transform the data for training and testing\ntrain_data['Fare'] = fareScaler.fit_transform(train_data[['Fare']].values.reshape(-1,1))\ntest_data['Fare'] = fareScaler.transform(test_data[['Fare']].values.reshape(-1,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age_normalized\", \"Fare\"]\n#features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age_normalized\", \"Fare\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=3, random_state=0).fit(X, y)\nsubmissionPredictions = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHECK FOR OVERFITTING\nfrom sklearn.metrics import accuracy_score\n\ntraining_predictions = model.predict(X)\ntraining_truth = y.to_numpy()\n\naccuracy = accuracy_score(training_truth, training_predictions)\nprint(f\"Accuracy:{accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CROSS-VALIDATION\n\ndef splitIntoGroups(data, numGroups):\n    df_shuffled = data.sample(frac=1, random_state=21).reset_index(drop=True)\n    groups = np.array_split(df_shuffled, 5)\n    return groups\n\ndef singleHyperTrain(train, test):\n    topAccuracy = {\"accuracy\": 0}\n    optimalWeights = []\n    estimators = [1, 10, 100, 1000, 10000]\n    max_depth = [1, 2, 3, 4, 5]\n    for i in estimators:\n        for j in max_depth:\n            y = train[\"Survived\"]\n            y_test = test[\"Survived\"]\n            # features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age_normalized\", \"Embarked\", \"Fare\", \"hasAge\"]\n            features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age_normalized\", \"Fare\"]\n            X = pd.get_dummies(train[features])\n            X_test = pd.get_dummies(test[features])\n            testModel = GradientBoostingClassifier(n_estimators=i, learning_rate=1.0, max_depth=j, random_state=0).fit(X, y)\n            predictions = testModel.predict(X_test)\n            accuracy = accuracy_score(y_test.to_numpy(), predictions)\n            if accuracy > topAccuracy[\"accuracy\"]:\n                topAccuracy = {\"accuracy\": accuracy, \"estimators\": i, \"max_depth\": j}\n            print(f\"{accuracy} with est: {i}, max depth: {j}\")\n    print(f\"top accuracy: {topAccuracy['accuracy']} with estimators = {topAccuracy['estimators']} and {topAccuracy['max_depth']} max depth\")\n        \ndef crossTrain(data):\n    for i in range(0, len(data)):\n        trainGroup = pd.concat([groupsOfData[j] for j in range(len(groupsOfData)) if j != i])\n        singleHyperTrain(trainGroup, groupsOfData[i])\n    \n        \ngroupsOfData = splitIntoGroups(train_data, 5)\ncrossTrain(groupsOfData)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE PREDICTIONS\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': submissionPredictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}