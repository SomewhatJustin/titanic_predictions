{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"name":"Titanic","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/somewhatjustin/titanic?scriptVersionId=164933266\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"'''\nTODO: \n- make cabin category more useful\n'''","metadata":{"id":"QO8b_fUVmrmC","execution":{"iopub.status.busy":"2024-03-01T01:19:49.818211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"KDTyvTYmmrmD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"id":"63jPsIoAmrmD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine parch and sibsp into fam\ntrain_data['fam'] = train_data['Parch']+train_data['SibSp']\ntest_data['fam'] = test_data['Parch']+test_data['SibSp']\n\ndef isAlone(fam):\n    if fam == 0:\n        return True\n    else:\n        return False\n\n# Check if alone\ntrain_data['alone'] = train_data['fam'].apply(isAlone)\ntest_data['alone'] = test_data['fam'].apply(isAlone)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Give Titles\ndef categorize_name(name):\n    if 'Mr.' in name:\n        return 'Mr'\n    elif 'Master' in name:\n        return 'Master'\n    elif 'Col.' in name or 'Major.' in name or 'Capt.' in name:\n        return 'Military'\n    elif 'Miss' in name or 'Ms.' in name or 'Mlle.' in name:\n        return 'Miss'\n    elif 'Mrs.' in name or 'Mme.' in name:\n        return 'Misses'\n    elif 'Rev.' in name:\n        return 'Reverend'\n    elif 'Dr.' in name:\n        return 'Doctor'\n    elif 'Don.' in name or 'Lady.' in name or 'Sir.' in name or 'Countess.' in name or 'Jonkheer.' in name:\n        return 'Royalty'\n    else:\n        return 'Other'\n\ntrain_data['Title'] = train_data['Name'].apply(categorize_name)\ntest_data['Title'] = test_data['Name'].apply(categorize_name)\n\ntrain_data[train_data['Title']==\"Other\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bucket Age\n\n# Fill NaNs without using inplace=True\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\n\n# Create age buckets\nage_buckets = np.linspace(0,80,20)\ntrain_data['Age_bucket'] = pd.cut(train_data['Age'], age_buckets)\ntest_data['Age_bucket'] = pd.cut(test_data['Age'], age_buckets)\n\nbuckets, blah = pd.factorize(train_data['Age_bucket'])\ntrain_data['Age_bucket'] = buckets\nbuckets, blah = pd.factorize(test_data['Age_bucket'])\ntest_data['Age_bucket'] = buckets\n\n#train_data['Age_normalized'] = scaler.fit_transform(train_data[['Age']].values.reshape(-1, 1))\n#test_data['Age_normalized'] = scaler.transform(test_data[['Age']].values.reshape(-1, 1))\n\ntrain_data.head(30)\n\n# Note: Use scaler.fit_transform() on the training data to fit the scaler and transform the data.\n# Use scaler.transform() on the test data to apply the same scaling based on the training data.","metadata":{"id":"pf-t9poQmrmD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fare Normalization\n\n# Fill NaNs without using inplace=True\ntrain_data['Fare'] = train_data['Fare'].fillna(train_data['Fare'].median())\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())","metadata":{"id":"b5PXYB0OmrmD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Passenger Class Normalization\n\nclass_mapping = {1: \"First\", 2: \"Second\", 3: \"Third\"}\ntrain_data['Pclass'] = train_data['Pclass'].map(class_mapping)\ntest_data['Pclass'] = test_data['Pclass'].map(class_mapping)","metadata":{"id":"FX8_c-NesAI8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sex binary\n\n# 2: female\n# 1: male\n\nsex_mapping = {\"male\": 1, \"female\": 2}\ntrain_data['Sex'] = train_data['Sex'].map(sex_mapping)\ntest_data['Sex'] = test_data['Sex'].map(sex_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cabin Classifier\n\ndef cabin_category(cabin):\n    # Check for missing cabin information\n    if pd.isnull(cabin):\n        return 'Other'\n    else:\n        # Search for specific letters in the cabin string\n        for letter in ['A', 'B', 'C', 'D', 'E', 'F']:\n            if letter in cabin:\n                return letter\n        # Return 'Other' if none of the specific letters are found\n        return 'Other'\n\n# Apply the function to create a new 'Cabin_Category' column\ntrain_data['Cabin_Category'] = train_data['Cabin'].apply(cabin_category)\ntest_data['Cabin_Category'] = test_data['Cabin'].apply(cabin_category)","metadata":{"id":"C1_ZV1_MvA8t","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysis\n\n# Now create a pivot table with the new 'Cabin_Category'\npivot_table = pd.crosstab(index=train_data['Survived'], columns=train_data['Cabin_Category'])\n\n# Display the pivot table\nprint(pivot_table)","metadata":{"id":"YZ6vcRvYtgyw","outputId":"71ce18f9-b142-4f33-ea8a-e6369b89612e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data[\"Survived\"]\n\n#features = [\"Pclass\", \"Sex\", \"Age_bucket\", \"Fare\", \"alone\", \"Embarked\", \"fam\", \"Cabin_Category\"]\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"alone\", \"Embarked\", \"fam\", \"Title\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\n\n# Ensure X_test has all the columns that X has, and fill missing ones with 0\nfor column in X.columns:\n    if column not in X_test.columns:\n        X_test[column] = 0  # Adding missing column in test data with default value of 0\n\n# Align the order of X_test columns to match X\nX_test = X_test[X.columns]\n\n#model = GradientBoostingClassifier(n_estimators=150, max_depth=3).fit(X, y)\nmodel = RandomForestClassifier(max_features=None, n_estimators=83).fit(X,y)\nsubmissionPredictions = model.predict(X_test)","metadata":{"id":"HrMaZwmWmrmD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHECK FOR OVERFITTING\nfrom sklearn.metrics import accuracy_score\n\ntraining_predictions = model.predict(X)\ntraining_truth = y.to_numpy()\n\naccuracy = accuracy_score(training_truth, training_predictions)\nprint(f\"Accuracy:{accuracy}\")","metadata":{"id":"G0Cn3XammrmD","outputId":"29c14863-b4cb-4961-c6e9-5f1e9d6034c0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CROSS-VALIDATION\nfrom sklearn.model_selection import GridSearchCV\n\nn_estimators = np.linspace(10, 100, 50, dtype=\"int\")\npara_grid = {'n_estimators': n_estimators, 'max_features': [\"sqrt\", \"log2\", None, 1, 2, 3, 4, 5, 6, 7, 8]}\n\nprint(n_estimators)\n\n#clf = GridSearchCV(estimator=model, param_grid=para_grid, cv=5, n_jobs=-1)\n#clf.fit(X,y)","metadata":{"id":"LMEH8kAGmrmE","outputId":"31a4e1d5-bd9e-4147-fca1-d4b64cecedcb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nsorted_keys = sorted(clf.cv_results_.keys())  # Uncomment after fitting\nprint(sorted_keys)  # Uncomment after fitting\n\n# Assuming `clf` is your GridSearchCV object and has been fitted with data\nbest_model = clf.best_estimator_\n\n# To get the best score achieved during the cross-validation\nbest_score = clf.best_score_\n\n# To see the parameters that yielded the best results\nbest_params = clf.best_params_\n\nprint(\"Best Score:\", best_score)\nprint(\"Best Parameters:\", best_params)\nprint(\"Best Model:\", best_model)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accessing feature importances\nfeature_importances = model.feature_importances_\n\n# Visualizing feature importances\nfeatures = X.columns\nindices = np.argsort(feature_importances)[::-1]\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature Importances in Gradient Boosting Classifier\")\nplt.bar(range(X.shape[1]), feature_importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), features[indices], rotation=90)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Importance\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE PREDICTIONS\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': submissionPredictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"id":"uELKBeK2mrmE","outputId":"51050a5e-5588-46df-f7eb-5fd47cf7c710","trusted":true},"execution_count":null,"outputs":[]}]}